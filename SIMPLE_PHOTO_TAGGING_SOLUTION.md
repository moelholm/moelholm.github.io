# Simplest AWS Rekognition Photo Tagging Solution

## üéØ The Absolute Simplest Approach

**No S3 bucket needed. No complex workflows. Just direct image bytes ‚Üí Rekognition ‚Üí JSON.**

## üí° Key Simplification

Instead of uploading to S3 first, **read photos directly from disk and send bytes to Rekognition**. This eliminates:
- S3 bucket setup
- S3 permissions/policies
- Upload/download/cleanup logic
- Additional moving parts

## üöÄ Complete Implementation

### 1. Python Script (Single File)

**File:** `tools/tag_photos_simple.py`

```python
#!/usr/bin/env python3
"""
Simplest AWS Rekognition photo tagging.
No S3 needed - sends image bytes directly to Rekognition.
"""
import os
import json
import hashlib
from pathlib import Path
import boto3

# Config
PHOTO_DIR = Path('site/img_running')
OUTPUT_FILE = Path('site/_data/photo_tags.json')
MAX_LABELS = 10
MIN_CONFIDENCE = 75

# AWS client
rekognition = boto3.client('rekognition')


def get_photo_hash(photo_path):
    """Hash photo for deduplication."""
    with open(photo_path, 'rb') as f:
        return hashlib.sha256(f.read()).hexdigest()[:12]


def tag_photo(photo_path):
    """Send photo bytes directly to Rekognition."""
    with open(photo_path, 'rb') as f:
        image_bytes = f.read()
    
    # Single API call - DetectLabels with quality info
    response = rekognition.detect_labels(
        Image={'Bytes': image_bytes},
        MaxLabels=MAX_LABELS,
        MinConfidence=MIN_CONFIDENCE,
        Features=['GENERAL_LABELS', 'IMAGE_PROPERTIES']
    )
    
    # Extract labels
    labels = [
        {'name': label['Name'], 'confidence': round(label['Confidence'], 1)}
        for label in response.get('Labels', [])
    ]
    
    # Extract quality score
    quality = 0
    if 'ImageProperties' in response:
        q = response['ImageProperties'].get('Quality', {})
        quality = round((q.get('Brightness', 50) + q.get('Sharpness', 50)) / 2, 1)
    
    return {
        'path': str(photo_path.relative_to('site')),
        'hash': get_photo_hash(photo_path),
        'labels': labels,
        'quality': quality
    }


def main():
    # Load existing tags
    existing = {}
    if OUTPUT_FILE.exists():
        with open(OUTPUT_FILE) as f:
            for item in json.load(f):
                existing[item['hash']] = item
    
    print(f"Found {len(existing)} previously tagged photos")
    
    # Find all photos
    photos = list(PHOTO_DIR.glob('*/*.jpg'))
    print(f"Found {len(photos)} total photos")
    
    # Process new photos only
    processed = 0
    for photo in photos:
        photo_hash = get_photo_hash(photo)
        
        if photo_hash in existing:
            continue  # Already processed
        
        print(f"Tagging: {photo.name}...", end=' ')
        
        try:
            result = tag_photo(photo)
            existing[photo_hash] = result
            processed += 1
            print(f"‚úì ({len(result['labels'])} labels)")
        except Exception as e:
            print(f"‚úó Error: {e}")
    
    print(f"\nProcessed {processed} new photos")
    
    # Save results
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(list(existing.values()), f, indent=2)
    
    print(f"Saved tags to {OUTPUT_FILE}")


if __name__ == '__main__':
    main()
```

### 2. GitHub Actions Workflow

**File:** `.github/workflows/tag-photos-simple.yml`

```yaml
name: Tag Photos (Simple)

on:
  workflow_dispatch:  # Manual trigger only
  schedule:
    - cron: '0 3 * * 0'  # Weekly Sunday 3am

permissions:
  contents: write
  id-token: write

jobs:
  tag:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install boto3
        run: pip install boto3
      
      - name: Configure AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN_MOELHOLM_WWW }}
          aws-region: eu-north-1
      
      - name: Tag photos
        run: python tools/tag_photos_simple.py
      
      - name: Commit results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add site/_data/photo_tags.json
          git diff --staged --quiet || git commit -m "Update photo tags [skip ci]"
          git push
```

### 3. IAM Policy (Minimal)

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "rekognition:DetectLabels",
      "Resource": "*"
    }
  ]
}
```

That's it. **No S3 bucket. No complex permissions. Just Rekognition.**

## üìä How It Works

```
GitHub Actions triggers weekly
    ‚Üì
Read photos from site/img_running/
    ‚Üì
For each new photo:
  - Calculate hash (check if already processed)
  - Read image bytes into memory
  - Send bytes directly to Rekognition API
  - Extract labels + quality score
  - Store in photo_tags.json
    ‚Üì
Commit photo_tags.json back to repo
    ‚Üì
Jekyll reads photo_tags.json at build time
```

## üí∞ Cost

- **DetectLabels**: $1.00 per 1,000 images
- **Your cost**: 
  - Initial: 580 photos = **$0.58**
  - Monthly: ~20 photos = **$0.02/month**
  - Annual: **$0.24/year**

**No S3 costs at all!**

## üîí Security

### Advantages of Direct Bytes
1. **No temp storage** - photos never leave GitHub runner
2. **No S3 bucket** - fewer attack surfaces
3. **Simpler permissions** - only need `rekognition:DetectLabels`
4. **No cleanup logic** - nothing to delete

### API Limits
- **Byte limit**: 5MB per image (your 600x600 JPEGs are ~100KB)
- **Rate limit**: 50 TPS (you'll process ~5 images/minute)
- **No throttling issues** for your volume

## üìù Example Output

**File:** `site/_data/photo_tags.json`

```json
[
  {
    "path": "img_running/2026-01-10/2026-01-10_872_small.jpg",
    "hash": "a3f5b8c9d2e1",
    "labels": [
      {"name": "Nature", "confidence": 99.8},
      {"name": "Snow", "confidence": 96.7},
      {"name": "Winter", "confidence": 95.3},
      {"name": "Trail", "confidence": 85.2}
    ],
    "quality": 87.3
  }
]
```

## üé® Display in Jekyll

**File:** `site/gallery-simple.md`

```markdown
---
layout: default
title: Photo Gallery
---

<h1>Photo Gallery</h1>

<div class="photo-grid">
{% for photo in site.data.photo_tags %}
  <div class="photo-item">
    <img src="/{{ photo.path }}" alt="{{ photo.labels[0].name }}">
    <div class="tags">
      {% for label in photo.labels limit:3 %}
        <span>{{ label.name }}</span>
      {% endfor %}
    </div>
  </div>
{% endfor %}
</div>
```

## ‚ö° Why This is Simpler

| Aspect | Complex (with S3) | Simple (direct bytes) |
|--------|-------------------|----------------------|
| **AWS Resources** | S3 bucket + Rekognition | Rekognition only |
| **IAM Permissions** | S3 + Rekognition (6 actions) | Rekognition only (1 action) |
| **Lines of Code** | ~300 | ~80 |
| **API Calls/Photo** | 4 (upload, analyze, download, delete) | 1 (analyze) |
| **Failure Points** | 5 (S3 upload, Rekognition, S3 cleanup, network x2) | 1 (Rekognition) |
| **Setup Steps** | 7 | 2 |
| **Monthly Cost** | $0.10 | $0.02 |

## üö¶ Deployment Steps

1. **Add IAM permission** to GitHub Actions role:
   ```json
   {
     "Effect": "Allow",
     "Action": "rekognition:DetectLabels",
     "Resource": "*"
   }
   ```

2. **Create the script**: `tools/tag_photos_simple.py`

3. **Create the workflow**: `.github/workflows/tag-photos-simple.yml`

4. **Test manually**:
   ```bash
   # Locally (with AWS credentials)
   python tools/tag_photos_simple.py
   
   # Or trigger GitHub Actions workflow
   ```

5. **Check results**: `site/_data/photo_tags.json`

6. **Build gallery page**: `site/gallery-simple.md`

Done! üéâ

## ü§î When to Use S3 Approach Instead

Use the S3 approach only if:
- You need to process images > 5MB (your images are ~100KB ‚úì)
- You want to reuse images for multiple Rekognition operations
- You need to archive analyzed images
- You're processing thousands of images in parallel

For your use case (580 photos, 600x600 JPEGs), **direct bytes is perfect**.

## üìà Performance

- **Processing speed**: ~5 photos/minute (rate limited by Rekognition)
- **Memory usage**: ~10MB (1 photo in memory at a time)
- **Network usage**: ~50MB upload (580 photos √ó 100KB each)
- **Runtime**: ~2 hours for initial 580 photos
- **Subsequent runs**: ~1 minute for 20 new photos

## üéØ Summary

**Simplest = Direct Bytes**

‚úÖ No S3 bucket  
‚úÖ No temp storage  
‚úÖ Minimal IAM permissions  
‚úÖ 80 lines of code  
‚úÖ $0.58 initial cost  
‚úÖ $0.24/year ongoing  
‚úÖ Secure by default  
‚úÖ Easy to understand  
‚úÖ Easy to debug  

Perfect for your static site architecture!
